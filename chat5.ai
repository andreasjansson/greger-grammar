## SYSTEM:

I have a parser in grammar.js and a scanner in src/scanner.c -- They're not producing the expected results.

I have a bunch of tests in ./test/corpus/*.txt

Run `tree-sitter test --help` to figure out how to test.

Make sure you download and read
* https://tree-sitter.github.io/tree-sitter/creating-parsers/2-the-grammar-dsl.html
* https://tree-sitter.github.io/tree-sitter/creating-parsers/3-writing-the-grammar.html
* https://tree-sitter.github.io/tree-sitter/creating-parsers/4-external-scanners.html
* https://tree-sitter.github.io/tree-sitter/creating-parsers/5-writing-tests.html

Take a look at some of the examples in ~/src/github/emacs-tree-sitter/tree-sitter-langs/repos/. The HTML grammar is especially interesting: ~/src/github/emacs-tree-sitter/tree-sitter-langs/repos/html/grammar.js, and its associated scanner too: ~/src/github/emacs-tree-sitter/tree-sitter-langs/repos/html/src/scanner.c -- make sure you read this!

## USER:

All tests pass except test/corpus/text-with-markdown-headings.txt. The issue is that text should be allowed to have headings in them, as long as it's not one of the known headers (e.g. `## USER:`).

Fix that.

## ASSISTANT:

I'll help you fix the issue with text containing markdown headings. Let me start by examining the current state of your parser and the failing test.

## TOOL USE:

Name: shell-command
ID: toolu_01LvDyGFYiGYw2DjpKDQKdbM

### command

<tool.toolu_01LvDyGFYiGYw2DjpKDQKdbM>
tree-sitter test --help
</tool.toolu_01LvDyGFYiGYw2DjpKDQKdbM>

## TOOL RESULT:

ID: toolu_01LvDyGFYiGYw2DjpKDQKdbM

<tool.toolu_01LvDyGFYiGYw2DjpKDQKdbM>
Command executed successfully:
Run a parser's tests

Usage: tree-sitter test [OPTIONS]

Options:
  -i, --include <INCLUDE>          Only run corpus test cases whose name matches the given regex
  -e, --exclude <EXCLUDE>          Only run corpus test cases whose name does not match the given regex
      --file-name <FILE_NAME>      Only run corpus test cases from from a given filename
  -u, --update                     Update all syntax trees in corpus files with current parser output
  -d, --debug                      Show parsing debug log
  -0, --debug-build                Compile a parser in debug mode
  -D, --debug-graph                Produce the log.html file with debug graphs
      --wasm                       Compile parsers to wasm instead of native dynamic libraries
      --open-log                   Open `log.html` in the default browser, if `--debug-graph` is supplied
      --config-path <CONFIG_PATH>  The path to an alternative config.json file
      --show-fields                Force showing fields in test diffs
      --stat <STAT>                Show parsing statistics [possible values: all, outliers-and-total, total-only]
  -r, --rebuild                    Force rebuild the parser
      --overview-only              Show only the pass-fail overview tree
  -h, --help                       Print help

</tool.toolu_01LvDyGFYiGYw2DjpKDQKdbM>

## TOOL USE:

Name: read-file
ID: toolu_019BH3zfNhD27CsBxdSiBTxr

### path

<tool.toolu_019BH3zfNhD27CsBxdSiBTxr>
test/corpus/text-with-markdown-headings.txt
</tool.toolu_019BH3zfNhD27CsBxdSiBTxr>

## TOOL RESULT:

ID: toolu_019BH3zfNhD27CsBxdSiBTxr

<tool.toolu_019BH3zfNhD27CsBxdSiBTxr>
===========================
Text with markdown headings
===========================

## USER:

Hello, how are you?

## This is not a Greger heading

test

# Another heading that should be part of text

---

(source_file
  (user
    (text)))
</tool.toolu_019BH3zfNhD27CsBxdSiBTxr>

## ASSISTANT:

Now let me examine the current grammar and scanner:

## TOOL USE:

Name: read-file
ID: toolu_01Rg3xoUt19GFQWDyDVzt8VU

### path

<tool.toolu_01Rg3xoUt19GFQWDyDVzt8VU>
grammar.js
</tool.toolu_01Rg3xoUt19GFQWDyDVzt8VU>

## TOOL RESULT:

ID: toolu_01Rg3xoUt19GFQWDyDVzt8VU

<tool.toolu_01Rg3xoUt19GFQWDyDVzt8VU>
/**
 * @file Greger grammar for tree-sitter
 * @author Andreas Jansson
 * @license MIT
 */

/// <reference types="tree-sitter-cli/dsl" />
// @ts-check

module.exports = grammar({
  name: "greger",

  extras: $ => [
    /[ \t\n]/,
  ],

  externals: $ => [
    $.tool_start_tag,
    $.tool_end_tag,
    $.tool_content,
    $.html_comment,
  ],



  inline: $ => [
    $.content_blocks,
  ],

  rules: {
    source_file: $ => repeat($._block),

    _block: $ => choice(
      $.user,
      $.assistant,
      $.system,
      $.thinking,
      $.tool_use,
      $.tool_result,
      $.server_tool_use,
      $.web_search_tool_result,
      $.citations,
    ),

    user: $ => seq(
      '##',
      'USER',
      ':',
      $.content_blocks,
    ),

    assistant: $ => seq(
      '##',
      'ASSISTANT',
      ':',
      $.content_blocks,
    ),

    system: $ => seq(
      '##',
      'SYSTEM',
      ':',
      $.content_blocks,
    ),

    thinking: $ => seq(
      '##',
      'THINKING',
      ':',
      $.content_blocks,
    ),

    tool_use: $ => seq(
      '##',
      'TOOL',
      'USE',
      ':',
      /\n/,
      optional(/\n/),
      repeat(choice(
        $.name,
        $.id,
        $.tool_param,
      )),
    ),

    tool_result: $ => seq(
      '##',
      'TOOL',
      'RESULT',
      ':',
      /\n/,
      optional(/\n/),
      $.id,
      optional(/\n/),
      $.content,
    ),

    server_tool_use: $ => seq(
      '##',
      'SERVER',
      'TOOL',
      'USE',
      ':',
      /\n/,
      optional(/\n/),
      repeat(choice(
        $.name,
        $.id,
        $.tool_param,
      )),
    ),

    web_search_tool_result: $ => seq(
      '##',
      'WEB',
      'SEARCH',
      'TOOL',
      'RESULT',
      ':',
      /\n/,
      optional(/\n/),
      $.id,
      optional(/\n/),
      $.content,
    ),

    citations: $ => seq(
      '##',
      'CITATIONS',
      ':',
      optional(alias($.citations_text, $.text)),
      repeat($.citation_entry),
    ),

    citations_text: $ => prec.right(repeat1(choice(
      $.cite_tag,
      $.safe_shell_commands,
      $._text_content,
    ))),

    name: $ => seq(
      'Name:',
      field('value', $.value),
      /\n/
    ),

    id: $ => seq(
      'ID:',
      field('value', $.value),
      /\n/
    ),

    tool_param: $ => seq(
      '###',
      /[ ]*/,
      alias($.param_name, $.name),
      /\n/,
      optional(/\n/),
      alias($._tool_element, $.value),
    ),

    param_name: $ => /[^\n]+/,

    citation_entry: $ => seq(
      '###',
      /[ ]*/,
      alias($.citation_url, $.url),
      /\n/,
      optional(/\n/),
      optional(alias($.citation_title, $.title)),
      optional(alias($.citation_text, $.cited_text)),
      optional(alias($.citation_encrypted_index, $.encrypted_index)),
    ),

    citation_url: $ => /[^\n]*/,

    citation_title: $ => seq(
      'Title:',
      /[ ]+/,
      field("value", $.value),
      /\n/,
    ),

    citation_text: $ => seq(
      'Cited text:',
      /[ ]+/,
      field("value", $.value),
      /\n/,
    ),

    citation_encrypted_index: $ => seq(
      'Encrypted index:',
      /[ ]+/,
      field("value", $.value),
      /\n/,
    ),

    value: _ => /[^\n]+/,

    content_blocks: $ => repeat1(choice(
      $.text,
      $.code_block,
      $.inline_code,
      $.html_comment,
    )),

    text: $ => prec.right(repeat1(choice(
      $.cite_tag,
      $.safe_shell_commands,
      $._text_content,
      /\n/,
    ))),

    _text_content: $ => token(prec(-1, /[^#<`\n]+/)),

    _tool_element: $ => seq(
      $.tool_start_tag,
      field('value', $.tool_content),
      $.tool_end_tag,
    ),

    content: $ => alias($._tool_element, 'content'),

    code_block: $ => seq(
      '```',
      optional(/[^\n]*/),
      /\n/,
      repeat(choice(
        /[^`\n]+/,
        /\n/,
        /`[^`]/,
        /``[^`]/,
      )),
      '```',
    ),

    inline_code: $ => seq(
      '`',
      /[^`\n]+/,
      '`',
    ),

    cite_tag: $ => seq(
      '<cite>',
      /[^<]*/,
      '</cite>',
    ),

    safe_shell_commands: $ => seq(
      '<safe-shell-commands>',
      repeat(choice(
        /[^<\n]+/,
        /\n/,
      )),
      '</safe-shell-commands>',
    ),
  }
});
</tool.toolu_01Rg3xoUt19GFQWDyDVzt8VU>

## TOOL USE:

Name: read-file
ID: toolu_01UhR4SBndFDywPrQeEuXbnQ

### path

<tool.toolu_01UhR4SBndFDywPrQeEuXbnQ>
src/scanner.c
</tool.toolu_01UhR4SBndFDywPrQeEuXbnQ>

## TOOL RESULT:

ID: toolu_01UhR4SBndFDywPrQeEuXbnQ

<tool.toolu_01UhR4SBndFDywPrQeEuXbnQ>
#include "tree_sitter/parser.h"
#include <wctype.h>
#include <string.h>
#include <stdio.h>

enum TokenType {
    TOOL_START_TAG,
    TOOL_END_TAG,
    TOOL_CONTENT,
    HTML_COMMENT,
};

typedef struct {
    char tool_id[256];
    bool in_tool_content;
} Scanner;

static inline void advance(TSLexer *lexer) { lexer->advance(lexer, false); }

static inline void skip(TSLexer *lexer) { lexer->advance(lexer, true); }

unsigned tree_sitter_greger_external_scanner_serialize(void *payload, char *buffer) {
    Scanner *scanner = (Scanner *)payload;

    size_t tool_id_len = strlen(scanner->tool_id);
    if (tool_id_len >= 255) tool_id_len = 255;

    buffer[0] = scanner->in_tool_content ? 1 : 0;
    buffer[1] = tool_id_len;
    memcpy(buffer + 2, scanner->tool_id, tool_id_len);

    return 2 + tool_id_len;
}

void tree_sitter_greger_external_scanner_deserialize(void *payload, const char *buffer, unsigned length) {
    Scanner *scanner = (Scanner *)payload;

    if (length == 0) {
        scanner->in_tool_content = false;
        scanner->tool_id[0] = '\0';
        return;
    }

    scanner->in_tool_content = buffer[0] == 1;
    if (length > 1) {
        unsigned tool_id_len = buffer[1];
        if (tool_id_len >= 255) tool_id_len = 255;
        if (length >= 2 + tool_id_len) {
            memcpy(scanner->tool_id, buffer + 2, tool_id_len);
            scanner->tool_id[tool_id_len] = '\0';
        } else {
            scanner->tool_id[0] = '\0';
        }
    } else {
        scanner->tool_id[0] = '\0';
    }
}

void *tree_sitter_greger_external_scanner_create(void) {
    Scanner *scanner = malloc(sizeof(Scanner));
    scanner->in_tool_content = false;
    scanner->tool_id[0] = '\0';
    return scanner;
}

void tree_sitter_greger_external_scanner_destroy(void *payload) {
    free(payload);
}

static bool scan_html_comment(TSLexer *lexer) {
    if (lexer->lookahead != '<') return false;
    advance(lexer);

    if (lexer->lookahead != '!') return false;
    advance(lexer);

    if (lexer->lookahead != '-') return false;
    advance(lexer);

    if (lexer->lookahead != '-') return false;
    advance(lexer);

    // Now we're inside the comment, scan until we find -->
    unsigned dashes = 0;
    while (lexer->lookahead) {
        switch (lexer->lookahead) {
            case '-':
                ++dashes;
                break;
            case '>':
                if (dashes >= 2) {
                    lexer->result_symbol = HTML_COMMENT;
                    advance(lexer);
                    return true;
                }
                // fallthrough
            default:
                dashes = 0;
        }
        advance(lexer);
    }

    return false;
}

static bool scan_tool_start_tag(Scanner *scanner, TSLexer *lexer) {
    if (lexer->lookahead != '<') return false;
    advance(lexer);

    // Check for "tool."
    if (lexer->lookahead != 't') return false;
    advance(lexer);
    if (lexer->lookahead != 'o') return false;
    advance(lexer);
    if (lexer->lookahead != 'o') return false;
    advance(lexer);
    if (lexer->lookahead != 'l') return false;
    advance(lexer);
    if (lexer->lookahead != '.') return false;
    advance(lexer);

    // Get the tool ID and store it
    int id_len = 0;
    while (lexer->lookahead != '>' && lexer->lookahead != 0 && id_len < 255) {
        scanner->tool_id[id_len++] = lexer->lookahead;
        advance(lexer);
    }
    scanner->tool_id[id_len] = '\0';

    if (lexer->lookahead != '>') return false;
    advance(lexer);

    scanner->in_tool_content = true;
    lexer->result_symbol = TOOL_START_TAG;
    return true;
}

static bool scan_tool_end_tag(Scanner *scanner, TSLexer *lexer) {
    if (lexer->lookahead != '<') return false;
    advance(lexer);

    if (lexer->lookahead != '/') return false;
    advance(lexer);

    // Check for "tool."
    if (lexer->lookahead != 't') return false;
    advance(lexer);
    if (lexer->lookahead != 'o') return false;
    advance(lexer);
    if (lexer->lookahead != 'o') return false;
    advance(lexer);
    if (lexer->lookahead != 'l') return false;
    advance(lexer);
    if (lexer->lookahead != '.') return false;
    advance(lexer);

    // Scan any tool ID until >
    while (lexer->lookahead != '>' && lexer->lookahead != 0) {
        advance(lexer);
    }

    if (lexer->lookahead != '>') return false;
    advance(lexer);

    scanner->in_tool_content = false;
    scanner->tool_id[0] = '\0';
    lexer->result_symbol = TOOL_END_TAG;
    return true;
}

static bool scan_tool_content(Scanner *scanner, TSLexer *lexer) {
    if (!scanner->in_tool_content) return false;

    lexer->mark_end(lexer);

    // Build the expected closing tag
    char expected_closing[512];
    snprintf(expected_closing, sizeof(expected_closing), "</tool.%s>", scanner->tool_id);
    int expected_len = strlen(expected_closing);

    int match_index = 0;
    bool has_content = false;

    // Scan until we find the closing tag
    while (lexer->lookahead != 0) {
        if (lexer->lookahead == expected_closing[match_index]) {
            match_index++;
            if (match_index == expected_len) {
                // Found complete closing tag, stop here (don't consume it)
                if (has_content) {
                    lexer->result_symbol = TOOL_CONTENT;
                    return true;
                } else {
                    // No content, let the grammar handle the end tag
                    return false;
                }
            }
            advance(lexer);
        } else {
            // Reset match and continue as content
            if (match_index > 0) {
                // We were partially matching, reset but don't advance yet
                match_index = 0;
                // Don't advance here, reprocess this character
            } else {
                advance(lexer);
                has_content = true;
                lexer->mark_end(lexer);
            }
        }
    }

    // Reached end without finding closing tag
    if (has_content) {
        lexer->result_symbol = TOOL_CONTENT;
        return true;
    }

    return false;
}

bool tree_sitter_greger_external_scanner_scan(void *payload, TSLexer *lexer, const bool *valid_symbols) {
    Scanner *scanner = (Scanner *)payload;

    // Handle tool content (raw text) when in tool content state
    if (valid_symbols[TOOL_CONTENT] && scanner->in_tool_content) {
        return scan_tool_content(scanner, lexer);
    }

    // Skip whitespace but preserve newlines for other tokens
    while (iswspace(lexer->lookahead) && lexer->lookahead != '\n') {
        skip(lexer);
    }

    if (lexer->lookahead == '<') {
        // Handle HTML comments first
        if (valid_symbols[HTML_COMMENT]) {
            return scan_html_comment(lexer);
        }

        // Handle tool start tag
        if (valid_symbols[TOOL_START_TAG]) {
            return scan_tool_start_tag(scanner, lexer);
        }

        // Handle tool end tag
        if (valid_symbols[TOOL_END_TAG]) {
            return scan_tool_end_tag(scanner, lexer);
        }
    }

    return false;
}
</tool.toolu_01UhR4SBndFDywPrQeEuXbnQ>

## ASSISTANT:

Now let me run the failing test to see the current output:

## TOOL USE:

Name: shell-command
ID: toolu_01FwAVua1pKDgBekGuZA2atr

### command

<tool.toolu_01FwAVua1pKDgBekGuZA2atr>
tree-sitter test --file-name text-with-markdown-headings.txt
</tool.toolu_01FwAVua1pKDgBekGuZA2atr>

## TOOL RESULT:

ID: toolu_01FwAVua1pKDgBekGuZA2atr

<tool.toolu_01FwAVua1pKDgBekGuZA2atr>
Command failed with exit code 1:   text-with-markdown-headings:
     16. ✗ [31mText with markdown headings[0m

1 failure:

correct / [32mexpected[0m / [31munexpected[0m

  1. Text with markdown headings:

    (source_file
      (user
[31m        (text))
[0m[31m      (ERROR
[0m[31m        (UNEXPECTED 'h')))[0m
[32m        (text)))[0m


</tool.toolu_01FwAVua1pKDgBekGuZA2atr>

## ASSISTANT:

Let me check what the actual input looks like and see what's happening. Let me run the test with debug mode:

## TOOL USE:

Name: shell-command
ID: toolu_01L3iZoWbTbYLzioovVowYBS

### command

<tool.toolu_01L3iZoWbTbYLzioovVowYBS>
tree-sitter test --file-name text-with-markdown-headings.txt --debug
</tool.toolu_01L3iZoWbTbYLzioovVowYBS>

## TOOL RESULT:

ID: toolu_01L3iZoWbTbYLzioovVowYBS

<tool.toolu_01L3iZoWbTbYLzioovVowYBS>
Command failed with exit code 1: new_parse
process version:0, version_count:1, state:1, row:0, col:0
lex_internal state:0, row:0, column:0
  skip character:10
  consume character:'#'
  consume character:'#'
lexed_lookahead sym:##, size:3
shift state:36
process version:0, version_count:1, state:36, row:1, col:2
lex_internal state:27, row:1, column:2
  skip character:' '
  consume character:'U'
  consume character:'S'
  consume character:'E'
  consume character:'R'
lexed_lookahead sym:USER, size:5
shift state:142
process version:0, version_count:1, state:142, row:1, col:7
lex_internal state:0, row:1, column:7
  consume character:':'
lexed_lookahead sym::, size:1
shift state:7
process version:0, version_count:1, state:7, row:1, col:8
lex_external state:2, row:1, column:8
lex_internal state:4, row:1, column:8
  consume character:10
  consume character:10
lexed_lookahead sym:tool_use_token1, size:2
shift state:15
process version:0, version_count:1, state:15, row:3, col:0
lex_external state:2, row:3, column:0
lex_internal state:157, row:3, column:0
  consume character:'H'
  consume character:'e'
  consume character:'l'
  consume character:'l'
  consume character:'o'
  consume character:','
  consume character:' '
  consume character:'h'
  consume character:'o'
  consume character:'w'
  consume character:' '
  consume character:'a'
  consume character:'r'
  consume character:'e'
  consume character:' '
  consume character:'y'
  consume character:'o'
  consume character:'u'
  consume character:'?'
lexed_lookahead sym:_text_content, size:19
shift state:14
process version:0, version_count:1, state:14, row:3, col:19
lex_external state:2, row:3, column:19
lex_internal state:157, row:3, column:19
  consume character:10
  consume character:10
lexed_lookahead sym:tool_use_token1, size:2
reduce sym:text_repeat1, child_count:2
shift state:14
process version:0, version_count:1, state:14, row:5, col:0
lex_external state:2, row:5, column:0
lex_internal state:157, row:5, column:0
  consume character:'#'
  consume character:'#'
lexed_lookahead sym:##, size:2
reduce sym:text_repeat1, child_count:2
reduce sym:text, child_count:1
reduce sym:user, child_count:4
shift state:36
process version:0, version_count:1, state:36, row:5, col:2
lex_internal state:27, row:5, column:2
  skip character:' '
  consume character:'T'
lex_external state:1, row:5, column:2
  skip character:' '
lex_internal state:0, row:5, column:2
  skip character:' '
  consume character:'T'
skip_unrecognized_character
lex_external state:1, row:5, column:4
lex_internal state:0, row:5, column:4
  consume character:'h'
lex_external state:1, row:5, column:5
lex_internal state:0, row:5, column:5
  consume character:'i'
lex_external state:1, row:5, column:6
lex_internal state:0, row:5, column:6
  consume character:'s'
lex_external state:1, row:5, column:7
  skip character:' '
lex_internal state:0, row:5, column:7
  skip character:' '
lex_external state:1, row:5, column:8
lex_internal state:0, row:5, column:8
  consume character:'i'
lex_external state:1, row:5, column:9
lex_internal state:0, row:5, column:9
  consume character:'s'
lex_external state:1, row:5, column:10
  skip character:' '
lex_internal state:0, row:5, column:10
  skip character:' '
lex_external state:1, row:5, column:11
lex_internal state:0, row:5, column:11
  consume character:'n'
lex_external state:1, row:5, column:12
lex_internal state:0, row:5, column:12
  consume character:'o'
lex_external state:1, row:5, column:13
lex_internal state:0, row:5, column:13
  consume character:'t'
lex_external state:1, row:5, column:14
  skip character:' '
lex_internal state:0, row:5, column:14
  skip character:' '
lex_external state:1, row:5, column:15
lex_internal state:0, row:5, column:15
  consume character:'a'
lex_external state:1, row:5, column:16
  skip character:' '
lex_internal state:0, row:5, column:16
  skip character:' '
lex_external state:1, row:5, column:17
lex_internal state:0, row:5, column:17
  consume character:'G'
lex_external state:1, row:5, column:18
lex_internal state:0, row:5, column:18
  consume character:'r'
lex_external state:1, row:5, column:19
lex_internal state:0, row:5, column:19
  consume character:'e'
lex_external state:1, row:5, column:20
lex_internal state:0, row:5, column:20
  consume character:'g'
lex_external state:1, row:5, column:21
lex_internal state:0, row:5, column:21
  consume character:'e'
lex_external state:1, row:5, column:22
lex_internal state:0, row:5, column:22
  consume character:'r'
lex_external state:1, row:5, column:23
  skip character:' '
lex_internal state:0, row:5, column:23
  skip character:' '
lex_external state:1, row:5, column:24
lex_internal state:0, row:5, column:24
  consume character:'h'
lex_external state:1, row:5, column:25
lex_internal state:0, row:5, column:25
  consume character:'e'
lex_external state:1, row:5, column:26
lex_internal state:0, row:5, column:26
  consume character:'a'
lex_external state:1, row:5, column:27
lex_internal state:0, row:5, column:27
  consume character:'d'
lex_external state:1, row:5, column:28
lex_internal state:0, row:5, column:28
  consume character:'i'
lex_external state:1, row:5, column:29
lex_internal state:0, row:5, column:29
  consume character:'n'
lex_external state:1, row:5, column:30
lex_internal state:0, row:5, column:30
  consume character:'g'
lex_external state:1, row:5, column:31
lex_internal state:0, row:5, column:31
  skip character:10
  skip character:10
lex_external state:1, row:7, column:0
lex_internal state:0, row:7, column:0
  consume character:'t'
lex_external state:1, row:7, column:1
lex_internal state:0, row:7, column:1
  consume character:'e'
lex_external state:1, row:7, column:2
lex_internal state:0, row:7, column:2
  consume character:'s'
lex_external state:1, row:7, column:3
lex_internal state:0, row:7, column:3
  consume character:'t'
lex_external state:1, row:7, column:4
lex_internal state:0, row:7, column:4
  skip character:10
  skip character:10
  consume character:'#'
lex_external state:1, row:9, column:1
  skip character:' '
lex_internal state:0, row:9, column:1
  skip character:' '
  consume character:'A'
lex_external state:1, row:9, column:3
lex_internal state:0, row:9, column:3
  consume character:'n'
lex_external state:1, row:9, column:4
lex_internal state:0, row:9, column:4
  consume character:'o'
lex_external state:1, row:9, column:5
lex_internal state:0, row:9, column:5
  consume character:'t'
lex_external state:1, row:9, column:6
lex_internal state:0, row:9, column:6
  consume character:'h'
lex_external state:1, row:9, column:7
lex_internal state:0, row:9, column:7
  consume character:'e'
lex_external state:1, row:9, column:8
lex_internal state:0, row:9, column:8
  consume character:'r'
lex_external state:1, row:9, column:9
  skip character:' '
lex_internal state:0, row:9, column:9
  skip character:' '
lex_external state:1, row:9, column:10
lex_internal state:0, row:9, column:10
  consume character:'h'
lex_external state:1, row:9, column:11
lex_internal state:0, row:9, column:11
  consume character:'e'
lex_external state:1, row:9, column:12
lex_internal state:0, row:9, column:12
  consume character:'a'
lex_external state:1, row:9, column:13
lex_internal state:0, row:9, column:13
  consume character:'d'
lex_external state:1, row:9, column:14
lex_internal state:0, row:9, column:14
  consume character:'i'
lex_external state:1, row:9, column:15
lex_internal state:0, row:9, column:15
  consume character:'n'
lex_external state:1, row:9, column:16
lex_internal state:0, row:9, column:16
  consume character:'g'
lex_external state:1, row:9, column:17
  skip character:' '
lex_internal state:0, row:9, column:17
  skip character:' '
lex_external state:1, row:9, column:18
lex_internal state:0, row:9, column:18
  consume character:'t'
lex_external state:1, row:9, column:19
lex_internal state:0, row:9, column:19
  consume character:'h'
lex_external state:1, row:9, column:20
lex_internal state:0, row:9, column:20
  consume character:'a'
lex_external state:1, row:9, column:21
lex_internal state:0, row:9, column:21
  consume character:'t'
lex_external state:1, row:9, column:22
  skip character:' '
lex_internal state:0, row:9, column:22
  skip character:' '
lex_external state:1, row:9, column:23
lex_internal state:0, row:9, column:23
  consume character:'s'
lex_external state:1, row:9, column:24
lex_internal state:0, row:9, column:24
  consume character:'h'
lex_external state:1, row:9, column:25
lex_internal state:0, row:9, column:25
  consume character:'o'
lex_external state:1, row:9, column:26
lex_internal state:0, row:9, column:26
  consume character:'u'
lex_external state:1, row:9, column:27
lex_internal state:0, row:9, column:27
  consume character:'l'
lex_external state:1, row:9, column:28
lex_internal state:0, row:9, column:28
  consume character:'d'
lex_external state:1, row:9, column:29
  skip character:' '
lex_internal state:0, row:9, column:29
  skip character:' '
lex_external state:1, row:9, column:30
lex_internal state:0, row:9, column:30
  consume character:'b'
lex_external state:1, row:9, column:31
lex_internal state:0, row:9, column:31
  consume character:'e'
lex_external state:1, row:9, column:32
  skip character:' '
lex_internal state:0, row:9, column:32
  skip character:' '
lex_external state:1, row:9, column:33
lex_internal state:0, row:9, column:33
  consume character:'p'
lex_external state:1, row:9, column:34
lex_internal state:0, row:9, column:34
  consume character:'a'
lex_external state:1, row:9, column:35
lex_internal state:0, row:9, column:35
  consume character:'r'
lex_external state:1, row:9, column:36
lex_internal state:0, row:9, column:36
  consume character:'t'
lex_external state:1, row:9, column:37
  skip character:' '
lex_internal state:0, row:9, column:37
  skip character:' '
lex_external state:1, row:9, column:38
lex_internal state:0, row:9, column:38
  consume character:'o'
lex_external state:1, row:9, column:39
lex_internal state:0, row:9, column:39
  consume character:'f'
lex_external state:1, row:9, column:40
  skip character:' '
lex_internal state:0, row:9, column:40
  skip character:' '
lex_external state:1, row:9, column:41
lex_internal state:0, row:9, column:41
  consume character:'t'
lex_external state:1, row:9, column:42
lex_internal state:0, row:9, column:42
  consume character:'e'
lex_external state:1, row:9, column:43
lex_internal state:0, row:9, column:43
  consume character:'x'
lex_external state:1, row:9, column:44
lex_internal state:0, row:9, column:44
  consume character:'t'
lex_external state:1, row:9, column:45
lex_internal state:0, row:9, column:45
  skip character:10
lexed_lookahead sym:ERROR, size:82
detect_error lookahead:ERROR
resume version:0
skip_token symbol:ERROR
process version:0, version_count:1, state:0, row:9, col:45
lex_external state:1, row:9, column:45
lex_internal state:0, row:9, column:45
  skip character:10
lexed_lookahead sym:end, size:1
recover_to_previous state:11, depth:3
recover_eof
process version:1, version_count:2, state:11, row:9, col:45
reduce sym:source_file, child_count:1
accept
select_smaller_error symbol:source_file, over_symbol:ERROR
done
  text-with-markdown-headings:
     16. ✗ [31mText with markdown headings[0m

1 failure:

correct / [32mexpected[0m / [31munexpected[0m

  1. Text with markdown headings:

    (source_file
      (user
[31m        (text))
[0m[31m      (ERROR
[0m[31m        (UNEXPECTED 'h')))[0m
[32m        (text)))[0m


</tool.toolu_01L3iZoWbTbYLzioovVowYBS>

## ASSISTANT:

I can see the issue! The problem is that when the parser encounters `## This is not a Greger heading`, it starts trying to parse it as a known heading (USER, ASSISTANT, etc.) but then fails because "This" is not one of the recognized keywords. The parser then goes into error recovery.

The issue is in the grammar - the `_text_content` rule excludes `#` characters, so when we have `## This is not a Greger heading`, the text parsing stops at the first `#`. Let me look at the tree-sitter documentation to understand how to handle this properly.

## TOOL USE:

Name: read-webpage
ID: toolu_018WUUKsY5zvwnyKbE6XH6BE

### url

<tool.toolu_018WUUKsY5zvwnyKbE6XH6BE>
https://tree-sitter.github.io/tree-sitter/creating-parsers/2-the-grammar-dsl.html
</tool.toolu_018WUUKsY5zvwnyKbE6XH6BE>

## TOOL RESULT:

ID: toolu_018WUUKsY5zvwnyKbE6XH6BE

<tool.toolu_018WUUKsY5zvwnyKbE6XH6BE>
    Keyboard shortcuts
   Press  ←  or  →  to navigate between chapters
  Press  S  or  /  to search in the book
  Press  ?  to show this help
  Press  Esc  to hide this help
 
 
 
    Work around some values being stored in localStorage wrapped in quotes     Set the theme before any content is loaded, prevents flash     Hide / unhide sidebar before it is displayed    populated by js        
 
    
            Auto
  Light
  Rust
  Coal
  Navy
  Ayu
 
    
  Tree-sitter
              
 
       
  

 
   Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM     The Grammar DSL
  The following is a complete list of built-in functions you can use in your  grammar.js  to define rules. Use-cases for some
of these functions will be explained in more detail in later sections.
   Symbols (the  $  object)  — Every grammar rule is written as a JavaScript function that takes a parameter conventionally
called  $ . The syntax  $.identifier  is how you refer to another grammar symbol within a rule. Names starting with  $.MISSING 
or  $.UNEXPECTED  should be avoided as they have special meaning for the  tree-sitter test  command.
 
   String and Regex literals  — The terminal symbols in a grammar are described using JavaScript strings and regular
expressions. Of course during parsing, Tree-sitter does not actually use JavaScript's regex engine to evaluate these regexes;
it generates its own regex-matching logic based on the Rust regex syntax as part of each parser. Regex literals are just
used as a convenient way of writing regular expressions in your grammar. You can use Rust regular expressions in your grammar
DSL through the  RustRegex  class. Simply pass your regex pattern as a string:
  new RustRegex('(?i)[a-z_][a-z0-9_]*') // matches a simple identifier

  Unlike JavaScript's builtin  RegExp  class, which takes a pattern and flags as separate arguments,  RustRegex  only
accepts a single pattern string. While it doesn't support separate flags, you can use inline flags within the pattern itself.
For more details about Rust's regex syntax and capabilities, check out the  Rust regex documentation .
     Note
 
   
   Only a subset of the Regex engine is actually supported. This is due to certain features like lookahead and lookaround
assertions not feasible to use in an LR(1) grammar, as well as certain flags being unnecessary for tree-sitter. However,
plenty of features are supported by default:
  Character classes
  Character ranges
  Character sets
  Quantifiers
  Alternation
  Grouping
  Unicode character escapes
  Unicode property escapes
 

 
 
   Sequences :  seq(rule1, rule2, ...)  — This function creates a rule that matches any number of other rules, one after
another. It is analogous to simply writing multiple symbols next to each other in  EBNF notation .
 
   Alternatives :  choice(rule1, rule2, ...)  — This function creates a rule that matches  one  of a set of possible
rules. The order of the arguments does not matter. This is analogous to the  |  (pipe) operator in EBNF notation.
 
   Repetitions :  repeat(rule)  — This function creates a rule that matches  zero-or-more  occurrences of a given rule.
It is analogous to the  {x}  (curly brace) syntax in EBNF notation.
 
   Repetitions :  repeat1(rule)  — This function creates a rule that matches  one-or-more  occurrences of a given rule.
The previous  repeat  rule is implemented in  repeat1  but is included because it is very commonly used.
 
   Options :  optional(rule)  — This function creates a rule that matches  zero or one  occurrence of a given rule.
It is analogous to the  [x]  (square bracket) syntax in EBNF notation.
 
   Precedence :  prec(number, rule)  — This function marks the given rule with a numerical precedence, which will be used
to resolve  LR(1) Conflicts  at parser-generation time. When two rules overlap in a way that represents either
a true ambiguity or a  local  ambiguity given one token of lookahead, Tree-sitter will try to resolve the conflict by matching
the rule with the higher precedence. The default precedence of all rules is zero. This works similarly to the
 precedence directives  in Yacc grammars.
  This function can also be used to assign lexical precedence to a given
token, but it must be wrapped in a  token  call, such as  token(prec(1, 'foo')) . This reads as "the token  foo  has a
lexical precedence of 1". The purpose of lexical precedence is to solve the issue where multiple tokens can match the same
set of characters, but one token should be preferred over the other. See  Lexical Precedence vs Parse Precedence 
for a more detailed explanation.
 
   Left Associativity :  prec.left([number], rule)  — This function marks the given rule as left-associative (and optionally
applies a numerical precedence). When an LR(1) conflict arises in which all the rules have the same numerical precedence,
Tree-sitter will consult the rules' associativity. If there is a left-associative rule, Tree-sitter will prefer matching
a rule that ends  earlier . This works similarly to  associativity directives  in Yacc grammars.
 
   Right Associativity :  prec.right([number], rule)  — This function is like  prec.left , but it instructs Tree-sitter
to prefer matching a rule that ends  later .
 
   Dynamic Precedence :  prec.dynamic(number, rule)  — This function is similar to  prec , but the given numerical precedence
is applied at  runtime  instead of at parser generation time. This is only necessary when handling a conflict dynamically
using the  conflicts  field in the grammar, and when there is a genuine  ambiguity : multiple rules correctly match a given
piece of code. In that event, Tree-sitter compares the total dynamic precedence associated with each rule, and selects the
one with the highest total. This is similar to  dynamic precedence directives  in Bison grammars.
 
   Tokens :  token(rule)  — This function marks the given rule as producing only
a single token. Tree-sitter's default is to treat each String or RegExp literal
in the grammar as a separate token. Each token is matched separately by the lexer
and returned as its own leaf node in the tree. The  token  function allows you to
express a complex rule using the functions described above (rather than as a single
regular expression) but still have Tree-sitter treat it as a single token.
The token function will only accept terminal rules, so  token($.foo)  will not work.
You can think of it as a shortcut for squashing complex rules of strings or regexes
down to a single token.
 
   Immediate Tokens :  token.immediate(rule)  — Usually, whitespace (and any other extras, such as comments) is optional
before each token. This function means that the token will only match if there is no whitespace.
 
   Aliases :  alias(rule, name)  — This function causes the given rule to  appear  with an alternative name in the syntax
tree. If  name  is a  symbol , as in  alias($.foo, $.bar) , then the aliased rule will  appear  as a  named node 
called  bar . And if  name  is a  string literal , as in  alias($.foo, 'bar') , then the aliased rule will appear as an
 anonymous node , as if the rule had been written as the simple string.
 
   Field Names :  field(name, rule)  — This function assigns a  field name  to the child node(s) matched by the given
rule. In the resulting syntax tree, you can then use that field name to access specific children.
 
   Reserved Keywords :  reserved(wordset, rule)   — This function will override the global reserved word set with the
one passed into the  wordset  parameter. This is useful for contextual keywords, such as  if  in JavaScript, which cannot
be used as a variable name in most contexts, but can be used as a property name.
 
 
 In addition to the  name  and  rules  fields, grammars have a few other optional public fields that influence the behavior
of the parser. Each of these fields is a function that accepts the grammar object ( $ ) as its only parameter, like the
grammar rules themselves. These fields are:
   extras  — an array of tokens that may appear  anywhere  in the language. This is often used for whitespace and
comments. The default value of  extras  is to accept whitespace. To control whitespace explicitly, specify
 extras: $ => []  in your grammar.
 
   inline  — an array of rule names that should be automatically  removed  from the grammar by replacing all of their
usages with a copy of their definition. This is useful for rules that are used in multiple places but for which you  don't 
want to create syntax tree nodes at runtime.
 
   conflicts  — an array of arrays of rule names. Each inner array represents a set of rules that's involved in an
 LR(1) conflict  that is  intended to exist  in the grammar. When these conflicts occur at runtime, Tree-sitter will use
the GLR algorithm to explore all the possible interpretations. If  multiple  parses end up succeeding, Tree-sitter will pick
the subtree whose corresponding rule has the highest total  dynamic precedence .
 
   externals  — an array of token names which can be returned by an
 external scanner . External scanners allow you to write custom C code which runs during the lexing
process to handle lexical rules (e.g. Python's indentation tokens) that cannot be described by regular expressions.
 
   precedences  — an array of arrays of strings, where each array of strings defines named precedence levels in descending
order. These names can be used in the  prec  functions to define precedence relative only to other names in the array, rather
than globally. Can only be used with parse precedence, not lexical precedence.
 
   word  — the name of a token that will match keywords to the
 keyword extraction  optimization.
 
   supertypes  — an array of hidden rule names which should be considered to be 'supertypes' in the generated
 node types  file .
 
   reserved  — similar in structure to the main  rules  property, an object of reserved word sets associated with an
array of reserved rules. The reserved rule in the array must be a terminal token meaning it must be a string, regex, or token,
or a terminal rule. The  first  reserved word set in the object is the global word set, meaning it applies to every rule
in every parse state. However, certain keywords are contextual, depending on the rule. For example, in JavaScript, keywords
are typically not allowed as ordinary variables, however, they  can  be used as a property name. In this situation, the  reserved 
function would be used, and the word set to pass in would be the name of the word set that is declared in the  reserved 
object that corresponds to an empty array, signifying  no  keywords are reserved.
 
 
  Mobile navigation buttons          
 
 
         
          Custom JS scripts  
 
</tool.toolu_018WUUKsY5zvwnyKbE6XH6BE>

## TOOL USE:

Name: read-webpage
ID: toolu_017FnVt7SY3BadFwniKZHbVz

### url

<tool.toolu_017FnVt7SY3BadFwniKZHbVz>
https://tree-sitter.github.io/tree-sitter/creating-parsers/3-writing-the-grammar.html
</tool.toolu_017FnVt7SY3BadFwniKZHbVz>

## TOOL RESULT:

ID: toolu_017FnVt7SY3BadFwniKZHbVz

<tool.toolu_017FnVt7SY3BadFwniKZHbVz>
    Keyboard shortcuts
   Press  ←  or  →  to navigate between chapters
  Press  S  or  /  to search in the book
  Press  ?  to show this help
  Press  Esc  to hide this help
 
 
 
    Work around some values being stored in localStorage wrapped in quotes     Set the theme before any content is loaded, prevents flash     Hide / unhide sidebar before it is displayed    populated by js        
 
    
            Auto
  Light
  Rust
  Coal
  Navy
  Ayu
 
    
  Tree-sitter
              
 
       
  

 
   Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM     Writing the Grammar
  Writing a grammar requires creativity. There are an infinite number of CFGs (context-free grammars) that can be used to describe
any given language. To produce a good Tree-sitter parser, you need to create a grammar with two important properties:
   An intuitive structure  — Tree-sitter's output is a  concrete syntax tree ; each node in the tree corresponds
directly to a  terminal or non-terminal symbol  in the grammar. So to produce an easy-to-analyze tree, there
should be a direct correspondence between the symbols in your grammar and the recognizable constructs in the language.
This might seem obvious, but it is very different from the way that context-free grammars are often written in contexts
like  language specifications  or  Yacc / Bison  parsers.
 
   A close adherence to LR(1)  — Tree-sitter is based on the  GLR parsing  algorithm. This means that while
it can handle any context-free grammar, it works most efficiently with a class of context-free grammars called  LR(1) Grammars .
In this respect, Tree-sitter's grammars are similar to (but less restrictive than)  Yacc  and  Bison  grammars,
but  different  from  ANTLR grammars ,  Parsing Expression Grammars , or the  ambiguous grammars 
commonly used in language specifications.
 
 
 It's unlikely that you'll be able to satisfy these two properties just by translating an existing context-free grammar directly
into Tree-sitter's grammar format. There are a few kinds of adjustments that are often required.
The following sections will explain these adjustments in more depth.
  The First Few Rules
  It's usually a good idea to find a formal specification for the language you're trying to parse. This specification will
most likely contain a context-free grammar. As you read through the rules of this CFG, you will probably discover a complex
and cyclic graph of relationships. It might be unclear how you should navigate this graph as you define your grammar.
  Although languages have very different constructs, their constructs can often be categorized in to similar groups like
 Declarations ,  Definitions ,  Statements ,  Expressions ,  Types  and  Patterns . In writing your grammar, a good first
step is to create just enough structure to include all of these basic  groups  of symbols. For a language like Go,
you might start with something like this:
  {
  // ...

  rules: {
    source_file: $ => repeat($._definition),

    _definition: $ => choice(
      $.function_definition
      // TODO: other kinds of definitions
    ),

    function_definition: $ => seq(
      'func',
      $.identifier,
      $.parameter_list,
      $._type,
      $.block
    ),

    parameter_list: $ => seq(
      '(',
       // TODO: parameters
      ')'
    ),

    _type: $ => choice(
      'bool'
      // TODO: other kinds of types
    ),

    block: $ => seq(
      '{',
      repeat($._statement),
      '}'
    ),

    _statement: $ => choice(
      $.return_statement
      // TODO: other kinds of statements
    ),

    return_statement: $ => seq(
      'return',
      $._expression,
      ';'
    ),

    _expression: $ => choice(
      $.identifier,
      $.number
      // TODO: other kinds of expressions
    ),

    identifier: $ => /[a-z]+/,

    number: $ => /\d+/
  }
}

  One important fact to know up front is that the start rule for the grammar is the first property in the  rules  object.
In the example above, that would correspond to  source_file , but it can be named anything.
  Some details of this grammar will be explained in more depth later on, but if you focus on the  TODO  comments, you can
see that the overall strategy is  breadth-first . Notably, this initial skeleton does not need to directly match an exact
subset of the context-free grammar in the language specification. It just needs to touch on the major groupings of rules
in as simple and obvious a way as possible.
  With this structure in place, you can now freely decide what part of the grammar to flesh out next. For example, you might
decide to start with  types . One-by-one, you could define the rules for writing basic types and composing them into more
complex types:
  {
  // ...

  _type: $ => choice(
    $.primitive_type,
    $.array_type,
    $.pointer_type
  ),

  primitive_type: $ => choice(
    'bool',
    'int'
  ),

  array_type: $ => seq(
    '[',
    ']',
    $._type
  ),

  pointer_type: $ => seq(
    '*',
    $._type
  )
}

  After developing the  type  sublanguage a bit further, you might decide to switch to working on  statements  or  expressions 
instead. It's often useful to check your progress by trying to parse some real code using  tree-sitter parse .
  And remember to add tests for each rule in your  test/corpus  folder!
  Structuring Rules Well
  Imagine that you were just starting work on the  Tree-sitter JavaScript parser . Naively, you might
try to directly mirror the structure of the  ECMAScript Language Spec . To illustrate the problem with this
approach, consider the following line of code:
  return x + y;

  According to the specification, this line is a  ReturnStatement , the fragment  x + y  is an  AdditiveExpression ,
and  x  and  y  are both  IdentifierReferences . The relationship between these constructs is captured by a complex series
of production rules:
  ReturnStatement          ->  'return' Expression
Expression               ->  AssignmentExpression
AssignmentExpression     ->  ConditionalExpression
ConditionalExpression    ->  LogicalORExpression
LogicalORExpression      ->  LogicalANDExpression
LogicalANDExpression     ->  BitwiseORExpression
BitwiseORExpression      ->  BitwiseXORExpression
BitwiseXORExpression     ->  BitwiseANDExpression
BitwiseANDExpression     ->  EqualityExpression
EqualityExpression       ->  RelationalExpression
RelationalExpression     ->  ShiftExpression
ShiftExpression          ->  AdditiveExpression
AdditiveExpression       ->  MultiplicativeExpression
MultiplicativeExpression ->  ExponentiationExpression
ExponentiationExpression ->  UnaryExpression
UnaryExpression          ->  UpdateExpression
UpdateExpression         ->  LeftHandSideExpression
LeftHandSideExpression   ->  NewExpression
NewExpression            ->  MemberExpression
MemberExpression         ->  PrimaryExpression
PrimaryExpression        ->  IdentifierReference

  The language spec encodes the twenty different precedence levels of JavaScript expressions using twenty levels of indirection
between  IdentifierReference  and  Expression . If we were to create a concrete syntax tree representing this statement
according to the language spec, it would have twenty levels of nesting, and it would contain nodes with names like  BitwiseXORExpression ,
which are unrelated to the actual code.
  Standard Rule Names
  Tree-sitter places no restrictions on how to name the rules of your grammar. It can be helpful, however, to follow certain conventions
used by many other established grammars in the ecosystem. Some of these well-established patterns are listed below:
  source_file : Represents an entire source file, this rule is commonly used as the root node for a grammar,
  expression / statement : Used to represent statements and expressions for a given language. Commonly defined as a choice between several
more specific sub-expression/sub-statement rules.
  block : Used as the parent node for block scopes, with its children representing the block's contents.
  type : Represents the types of a language such as  int ,  char , and  void .
  identifier : Used for constructs like variable names, function arguments, and object fields; this rule is commonly used as the  word 
token in grammars.
  string : Used to represent  "string literals" .
  comment : Used to represent comments, this rule is commonly used as an  extra .
 
 Using Precedence
  To produce a readable syntax tree, we'd like to model JavaScript expressions using a much flatter structure like this:
  {
  // ...

  _expression: $ => choice(
    $.identifier,
    $.unary_expression,
    $.binary_expression,
    // ...
  ),

  unary_expression: $ => choice(
    seq('-', $._expression),
    seq('!', $._expression),
    // ...
  ),

  binary_expression: $ => choice(
    seq($._expression, '*', $._expression),
    seq($._expression, '+', $._expression),
    // ...
  ),
}

  Of course, this flat structure is highly ambiguous. If we try to generate a parser, Tree-sitter gives us an error message:
  Error: Unresolved conflict for symbol sequence:

  '-'  _expression  •  '*'  …

Possible interpretations:

  1:  '-'  (binary_expression  _expression  •  '*'  _expression)
  2:  (unary_expression  '-'  _expression)  •  '*'  …

Possible resolutions:

  1:  Specify a higher precedence in `binary_expression` than in the other rules.
  2:  Specify a higher precedence in `unary_expression` than in the other rules.
  3:  Specify a left or right associativity in `unary_expression`
  4:  Add a conflict for these rules: `binary_expression` `unary_expression`

     Hint
 
   
   The • character in the error message indicates where exactly during
parsing the conflict occurs, or in other words, where the parser is encountering
ambiguity.
 
 
  For an expression like  -a * b , it's not clear whether the  -  operator applies to the  a * b  or just to the  a . This
is where the  prec  function  described in the previous page  comes into play. By wrapping a rule with  prec ,
we can indicate that certain sequence of symbols should  bind to each other more tightly  than others. For example, the
 '-', $._expression  sequence in  unary_expression  should bind more tightly than the  $._expression, '+', $._expression 
sequence in  binary_expression :
  {
  // ...

  unary_expression: $ =>
    prec(
      2,
      choice(
        seq("-", $._expression),
        seq("!", $._expression),
        // ...
      ),
    );
}

  Using Associativity
  Applying a higher precedence in  unary_expression  fixes that conflict, but there is still another conflict:
  Error: Unresolved conflict for symbol sequence:

  _expression  '*'  _expression  •  '*'  …

Possible interpretations:

  1:  _expression  '*'  (binary_expression  _expression  •  '*'  _expression)
  2:  (binary_expression  _expression  '*'  _expression)  •  '*'  …

Possible resolutions:

  1:  Specify a left or right associativity in `binary_expression`
  2:  Add a conflict for these rules: `binary_expression`

  For an expression like  a * b * c , it's not clear whether we mean  a * (b * c)  or  (a * b) * c .
This is where  prec.left  and  prec.right  come into use. We want to select the second interpretation, so we use  prec.left .
  {
  // ...

  binary_expression: $ => choice(
    prec.left(2, seq($._expression, '*', $._expression)),
    prec.left(1, seq($._expression, '+', $._expression)),
    // ...
  ),
}

  Using Conflicts
  Sometimes, conflicts are actually desirable. In our JavaScript grammar, expressions and patterns can create intentional ambiguity.
A construct like  [x, y]  could be legitimately parsed as both an array literal (like in  let a = [x, y] ) or as a destructuring
pattern (like in  let [x, y] = arr ).
  module.exports = grammar({
  name: "javascript",

  rules: {
    expression: $ => choice(
      $.identifier,
      $.array,
      $.pattern,
    ),

    array: $ => seq(
      "[",
      optional(seq(
        $.expression, repeat(seq(",", $.expression))
      )),
      "]"
    ),

    array_pattern: $ => seq(
      "[",
      optional(seq(
        $.pattern, repeat(seq(",", $.pattern))
      )),
      "]"
    ),

    pattern: $ => choice(
      $.identifier,
      $.array_pattern,
    ),
  },
})

  In such cases, we want the parser to explore both possibilities by explicitly declaring this ambiguity:
  {
  name: "javascript",

  conflicts: $ => [
    [$.array, $.array_pattern],
  ],

  rules: {
    // ...
  },
}

     Note
 
   
   The example is a bit contrived for the purpose of illustrating the usage of conflicts. The actual JavaScript grammar isn't
structured like that, but this conflict is actually present in the
 Tree-sitter JavaScript grammar .
 
 
  Hiding Rules
  You may have noticed in the above examples that some grammar rule name like  _expression  and  _type  began with an underscore.
Starting a rule's name with an underscore causes the rule to be  hidden  in the syntax tree. This is useful for rules like
 _expression  in the grammars above, which always just wrap a single child node. If these nodes were not hidden, they would
add substantial depth and noise to the syntax tree without making it any easier to understand.
  Using Fields
  Often, it's easier to analyze a syntax node if you can refer to its children by  name  instead of by their position in an
ordered list. Tree-sitter grammars support this using the  field  function. This function allows you to assign unique names
to some or all of a node's children:
  function_definition: $ =>
  seq(
    "func",
    field("name", $.identifier),
    field("parameters", $.parameter_list),
    field("return_type", $._type),
    field("body", $.block),
  );

  Adding fields like this allows you to retrieve nodes using the  field APIs .
  Lexical Analysis
  Tree-sitter's parsing process is divided into two phases: parsing (which is described above) and  lexing  — the
process of grouping individual characters into the language's fundamental  tokens . There are a few important things to
know about how Tree-sitter's lexing works.
  Conflicting Tokens
  Grammars often contain multiple tokens that can match the same characters. For example, a grammar might contain the tokens
( "if"  and  /[a-z]+/ ). Tree-sitter differentiates between these conflicting tokens in a few ways.
   Context-aware Lexing  — Tree-sitter performs lexing on-demand, during the parsing process. At any given position
in a source document, the lexer only tries to recognize tokens that are  valid  at that position in the document.
 
   Lexical Precedence  — When the precedence functions described  in the previous page  are used  within 
the  token  function, the given explicit precedence values serve as instructions to the lexer. If there are two valid tokens
that match the characters at a given position in the document, Tree-sitter will select the one with the higher precedence.
 
   Match Length  — If multiple valid tokens with the same precedence match the characters at a given position in a document,
Tree-sitter will select the token that matches the  longest sequence of characters .
 
   Match Specificity  — If there are two valid tokens with the same precedence, and they both match the same number
of characters, Tree-sitter will prefer a token that is specified in the grammar as a  String  over a token specified as
a  RegExp .
 
   Rule Order  — If none of the above criteria can be used to select one token over another, Tree-sitter will prefer
the token that appears earlier in the grammar.
 
 
 If there is an external scanner it may have  an additional impact  over regular tokens
defined in the grammar.
  Lexical Precedence vs. Parse Precedence
  One common mistake involves not distinguishing  lexical precedence  from  parse precedence . Parse precedence determines
which rule is chosen to interpret a given sequence of tokens.  Lexical precedence  determines which token is chosen to interpret
at a given position of text, and it is a lower-level operation that is done first. The above list fully captures Tree-sitter's
lexical precedence rules, and you will probably refer back to this section of the documentation more often than any other.
Most of the time when you really get stuck, you're dealing with a lexical precedence problem. Pay particular attention to
the difference in meaning between using  prec  inside the  token  function versus outside it. The  lexical precedence  syntax,
as mentioned in the previous page, is  token(prec(N, ...)) .
  Keywords
  Many languages have a set of  keyword  tokens (e.g.  if ,  for ,  return ), as well as a more general token (e.g.  identifier )
that matches any word, including many of the keyword strings. For example, JavaScript has a keyword  instanceof , which is
used as a binary operator, like this:
  if (a instanceof Something) b();

  The following, however, is not valid JavaScript:
  if (a instanceofSomething) b();

  A keyword like  instanceof  cannot be followed immediately by another letter, because then it would be tokenized as an  identifier ,
 even though an identifier is not valid at that position . Because Tree-sitter uses context-aware lexing, as described
 above , it would not normally impose this restriction. By default, Tree-sitter would recognize  instanceofSomething 
as two separate tokens: the  instanceof  keyword followed by an  identifier .
  Keyword Extraction
  Fortunately, Tree-sitter has a feature that allows you to fix this, so that you can match the behavior of other standard
parsers: the  word  token. If you specify a  word  token in your grammar, Tree-sitter will find the set of  keyword  tokens
that match strings also matched by the  word  token. Then, during lexing, instead of matching each of these keywords individually,
Tree-sitter will match the keywords via a two-step process where it  first  matches the  word  token.
  For example, suppose we added  identifier  as the  word  token in our JavaScript grammar:
  grammar({
  name: "javascript",

  word: $ => $.identifier,

  rules: {
    _expression: $ =>
      choice(
        $.identifier,
        $.unary_expression,
        $.binary_expression,
        // ...
      ),

    binary_expression: $ =>
      choice(
        prec.left(1, seq($._expression, "instanceof", $._expression)),
        // ...
      ),

    unary_expression: $ =>
      choice(
        prec.left(2, seq("typeof", $._expression)),
        // ...
      ),

    identifier: $ => /[a-z_]+/,
  },
});

  Tree-sitter would identify  typeof  and  instanceof  as keywords. Then, when parsing the invalid code above, rather than
scanning for the  instanceof  token individually, it would scan for an  identifier  first, and find  instanceofSomething .
It would then correctly recognize the code as invalid.
  Aside from improving error detection, keyword extraction also has performance benefits. It allows Tree-sitter to generate
a smaller, simpler lexing function, which means that  the parser will compile much more quickly .
     Note
 
   
   The word token must be a unique token that is not reused by another rule. If you want to have a word token used in a
rule that's called something else, you should just alias the word token instead, like how the Rust grammar does it
 here
 
 
   Mobile navigation buttons          
 
 
         
          Custom JS scripts  
 
</tool.toolu_017FnVt7SY3BadFwniKZHbVz>

## ASSISTANT:

Now I understand the issue better. The problem is that when the parser sees `##`, it expects it to be followed by one of the known keywords (USER, ASSISTANT, SYSTEM, etc.). But when it's followed by something else like "This", it fails.

Let me examine the HTML grammar to see how they handle this:

## TOOL USE:

Name: read-file
ID: toolu_01CnX2S6oXnjCTyWky57NEko

### path

<tool.toolu_01CnX2S6oXnjCTyWky57NEko>
~/src/github/emacs-tree-sitter/tree-sitter-langs/repos/html/grammar.js
</tool.toolu_01CnX2S6oXnjCTyWky57NEko>

## TOOL RESULT:

ID: toolu_01CnX2S6oXnjCTyWky57NEko

<tool.toolu_01CnX2S6oXnjCTyWky57NEko>
/**
 * @file HTML grammar for tree-sitter
 * @author Max Brunsfeld <maxbrunsfeld@gmail.com>
 * @author Amaan Qureshi <amaanq12@gmail.com>
 * @license MIT
 */

/// <reference types="tree-sitter-cli/dsl" />
// @ts-check

module.exports = grammar({
  name: 'html',

  extras: $ => [
    $.comment,
    /\s+/,
  ],

  conflicts: [
    [$.text_content],
  ],

  externals: $ => [
    $._start_tag_name,
    $._script_start_tag_name,
    $._style_start_tag_name,
    $._end_tag_name,
    $.erroneous_end_tag_name,
    '/>',
    $._implicit_end_tag,
    $.raw_text,
    $.comment,
  ],

  rules: {
    document: $ => repeat($._node),

    doctype: $ => seq(
      '<!',
      alias($._doctype, 'doctype'),
      /[^>]+/,
      '>',
    ),

    _doctype: _ => /[Dd][Oo][Cc][Tt][Yy][Pp][Ee]/,

    _node: $ => choice(
      $.doctype,
      $.entity,
      $.text,
      $.element,
      $.script_element,
      $.style_element,
      $.erroneous_end_tag,
    ),

    element: $ => choice(
      seq(
        $.start_tag,
        repeat($._node),
        choice($.end_tag, $._implicit_end_tag),
      ),
      $.self_closing_tag,
    ),

    script_element: $ => seq(
      alias($.script_start_tag, $.start_tag),
      optional($.raw_text),
      $.end_tag,
    ),

    style_element: $ => seq(
      alias($.style_start_tag, $.start_tag),
      optional($.raw_text),
      $.end_tag,
    ),

    start_tag: $ => seq(
      '<',
      alias($._start_tag_name, $.tag_name),
      repeat($.attribute),
      '>',
    ),

    script_start_tag: $ => seq(
      '<',
      alias($._script_start_tag_name, $.tag_name),
      repeat($.attribute),
      '>',
    ),

    style_start_tag: $ => seq(
      '<',
      alias($._style_start_tag_name, $.tag_name),
      repeat($.attribute),
      '>',
    ),

    self_closing_tag: $ => seq(
      '<',
      alias($._start_tag_name, $.tag_name),
      repeat($.attribute),
      '/>',
    ),

    end_tag: $ => seq(
      '</',
      alias($._end_tag_name, $.tag_name),
      '>',
    ),

    erroneous_end_tag: $ => seq(
      '</',
      $.erroneous_end_tag_name,
      '>',
    ),

    attribute: $ => seq(
      $.attribute_name,
      optional(seq(
        '=',
        choice(
          $.attribute_value,
          $.quoted_attribute_value,
        ),
      )),
    ),

    attribute_name: _ => /[^<>"'/=\s]+/,

    attribute_value: _ => /[^<>"'=\s]+/,

    // An entity can be named, numeric (decimal), or numeric (hexacecimal). The
    // longest entity name is 29 characters long, and the HTML spec says that
    // no more will ever be added.
    entity: _ => /&(#([xX][0-9a-fA-F]{1,6}|[0-9]{1,5})|[A-Za-z]{1,30});?/,

    quoted_attribute_value: $ => choice(
      seq('\'', optional(alias(/[^']+/, $.attribute_value)), '\''),
      seq('"', optional(alias(/[^"]+/, $.attribute_value)), '"'),
    ),

    text: _ => /[^<>&\s]([^<>&]*[^<>&\s])?/,
  },
});
</tool.toolu_01CnX2S6oXnjCTyWky57NEko>

## ASSISTANT:

The HTML grammar doesn't have the same issue since it doesn't try to match specific tag patterns. Let me look at the problem from a different angle. The issue is that the current grammar is too strict - it tries to match `##` followed by specific keywords, but we want to allow text that contains `##` if it's not followed by the specific keywords we care about.

The solution is to modify the `_text_content` rule to allow `#` characters in some contexts. Let me check what happens if we modify the text content rule to be more permissive:

## TOOL USE:

Name: str-replace
ID: toolu_014qEn957QBp2dgTSx3G18hX

### file-path

<tool.toolu_014qEn957QBp2dgTSx3G18hX>
grammar.js
</tool.toolu_014qEn957QBp2dgTSx3G18hX>

### original-content

<tool.toolu_014qEn957QBp2dgTSx3G18hX>
    text: $ => prec.right(repeat1(choice(
      $.cite_tag,
      $.safe_shell_commands,
      $._text_content,
      /\n/,
    ))),

    _text_content: $ => token(prec(-1, /[^#<`\n]+/)),
</tool.toolu_014qEn957QBp2dgTSx3G18hX>

### new-content

<tool.toolu_014qEn957QBp2dgTSx3G18hX>
    text: $ => prec.right(repeat1(choice(
      $.cite_tag,
      $.safe_shell_commands,
      $._text_content,
      $._hash_content,
      /\n/,
    ))),

    _text_content: $ => token(prec(-1, /[^#<`\n]+/)),

    _hash_content: $ => token(prec(-2, /#[^#\n]*/)),
</tool.toolu_014qEn957QBp2dgTSx3G18hX>

### git-commit-message

<tool.toolu_014qEn957QBp2dgTSx3G18hX>
Add hash content rule to allow # in text
</tool.toolu_014qEn957QBp2dgTSx3G18hX>

<!-- TOOL_RESULT_PLACEHOLDER_toolu_014qEn957QBp2dgTSx3G18hX -->